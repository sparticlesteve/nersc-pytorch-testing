
Lmod is automatically replacing "nvidia/20.9" with "gcc/9.3.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.4

4: Distributed init with master addr 10.100.0.24 port 29500
6: Distributed init with master addr 10.100.0.24 port 29500
7: Distributed init with master addr 10.100.0.24 port 29500
5: Distributed init with master addr 10.100.0.24 port 29500
1: Distributed init with master addr 10.100.0.24 port 29500
3: Distributed init with master addr 10.100.0.24 port 29500
0: Distributed init with master addr 10.100.0.24 port 29500
2: Distributed init with master addr 10.100.0.24 port 29500
5: Initialized host nid001029 rank 5 local-rank 1 size 8
2: Initialized host nid001028 rank 2 local-rank 2 size 8
1: Initialized host nid001028 rank 1 local-rank 1 size 8
6: Initialized host nid001029 rank 6 local-rank 2 size 8
3: Initialized host nid001028 rank 3 local-rank 3 size 8
5: cuda:1
5: Generating a batch of data
0: Initialized host nid001028 rank 0 local-rank 0 size 8
4: Initialized host nid001029 rank 4 local-rank 0 size 8
6: cuda:2
6: Generating a batch of data
7: Initialized host nid001029 rank 7 local-rank 3 size 8
2: cuda:2
2: Generating a batch of data
1: cuda:1
1: Generating a batch of data
3: cuda:3
3: Generating a batch of data
4: cuda:0
4: Generating a batch of data
0: cuda:0
0: Generating a batch of data
7: cuda:3
7: Generating a batch of data
1: Constructing model
2: Constructing model
3: Constructing model
7: Constructing model
0: Constructing model
4: Constructing model
6: Constructing model
5: Constructing model
0: nid001028:129494:129494 [0] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.186<0> [1]hsn0:10.249.3.185<0> [2]hsn0:border:128.55.66.37<0>
0: nid001028:129494:129494 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
0: nid001028:129494:129494 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.186<0>
0: nid001028:129494:129494 [0] NCCL INFO Using network IB
0: NCCL version 2.7.8+cuda11.1
1: nid001028:129495:129495 [1] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.186<0> [1]hsn0:10.249.3.185<0> [2]hsn0:border:128.55.66.37<0>
1: nid001028:129495:129495 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
3: nid001028:129497:129497 [3] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.186<0> [1]hsn0:10.249.3.185<0> [2]hsn0:border:128.55.66.37<0>
3: nid001028:129497:129497 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
2: nid001028:129496:129496 [2] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.186<0> [1]hsn0:10.249.3.185<0> [2]hsn0:border:128.55.66.37<0>
2: nid001028:129496:129496 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
1: nid001028:129495:129495 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.186<0>
1: nid001028:129495:129495 [1] NCCL INFO Using network IB
3: nid001028:129497:129497 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.186<0>
3: nid001028:129497:129497 [3] NCCL INFO Using network IB
2: nid001028:129496:129496 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.186<0>
2: nid001028:129496:129496 [2] NCCL INFO Using network IB
4: nid001029:130775:130775 [0] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.218<0> [1]hsn0:10.249.3.217<0> [2]hsn0:border:128.55.70.50<0>
7: nid001029:130778:130778 [3] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.218<0> [1]hsn0:10.249.3.217<0> [2]hsn0:border:128.55.70.50<0>
4: nid001029:130775:130775 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
7: nid001029:130778:130778 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
5: nid001029:130776:130776 [1] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.218<0> [1]hsn0:10.249.3.217<0> [2]hsn0:border:128.55.70.50<0>
5: nid001029:130776:130776 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
7: nid001029:130778:130778 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.218<0>
7: nid001029:130778:130778 [3] NCCL INFO Using network IB
5: nid001029:130776:130776 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.218<0>
5: nid001029:130776:130776 [1] NCCL INFO Using network IB
4: nid001029:130775:130775 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.218<0>
4: nid001029:130775:130775 [0] NCCL INFO Using network IB
6: nid001029:130777:130777 [2] NCCL INFO Bootstrap : Using [0]hsn1:10.249.3.218<0> [1]hsn0:10.249.3.217<0> [2]hsn0:border:128.55.70.50<0>
6: nid001029:130777:130777 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
6: nid001029:130777:130777 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE ; OOB hsn1:10.249.3.218<0>
6: nid001029:130777:130777 [2] NCCL INFO Using network IB
3: nid001028:129497:129564 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
3: nid001028:129497:129564 [3] NCCL INFO Trees [0] -1/-1/-1->3->2|2->3->-1/-1/-1 [1] -1/-1/-1->3->2|2->3->-1/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1
3: nid001028:129497:129564 [3] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
4: nid001029:130775:130842 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
4: nid001029:130775:130842 [0] NCCL INFO Trees [0] 5/-1/-1->4->1|1->4->5/-1/-1 [1] 5/-1/-1->4->1|1->4->5/-1/-1 [2] 5/-1/-1->4->-1|-1->4->5/-1/-1 [3] 5/-1/-1->4->-1|-1->4->5/-1/-1
4: nid001029:130775:130842 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
0: nid001028:129494:129556 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7
0: nid001028:129494:129556 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7
0: nid001028:129494:129556 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7
0: nid001028:129494:129556 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7
1: nid001028:129495:129563 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
1: nid001028:129495:129563 [1] NCCL INFO Trees [0] 2/4/-1->1->0|0->1->2/4/-1 [1] 2/4/-1->1->0|0->1->2/4/-1 [2] 2/-1/-1->1->0|0->1->2/-1/-1 [3] 2/-1/-1->1->0|0->1->2/-1/-1
1: nid001028:129495:129563 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
5: nid001029:130776:130843 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
5: nid001029:130776:130843 [1] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/-1/-1->5->4|4->5->6/-1/-1 [2] 6/0/-1->5->4|4->5->6/0/-1 [3] 6/0/-1->5->4|4->5->6/0/-1
5: nid001029:130776:130843 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
2: nid001028:129496:129565 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
2: nid001028:129496:129565 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1
2: nid001028:129496:129565 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
6: nid001029:130777:130847 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
6: nid001029:130777:130847 [2] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 7/-1/-1->6->5|5->6->7/-1/-1 [3] 7/-1/-1->6->5|5->6->7/-1/-1
6: nid001029:130777:130847 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
0: nid001028:129494:129556 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
0: nid001028:129494:129556 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1 [2] 1/-1/-1->0->5|5->0->1/-1/-1 [3] 1/-1/-1->0->5|5->0->1/-1/-1
7: nid001029:130778:130844 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
7: nid001029:130778:130844 [3] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1 [2] -1/-1/-1->7->6|6->7->-1/-1/-1 [3] -1/-1/-1->7->6|6->7->-1/-1/-1
7: nid001029:130778:130844 [3] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
0: nid001028:129494:129556 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
1: nid001028:129495:129563 [1] NCCL INFO Channel 00 : 1[41000] -> 2[81000] via P2P/IPC/read
2: nid001028:129496:129565 [2] NCCL INFO Channel 00 : 2[81000] -> 3[c1000] via P2P/IPC/read
6: nid001029:130777:130847 [2] NCCL INFO Channel 00 : 6[81000] -> 7[c1000] via P2P/IPC/read
5: nid001029:130776:130843 [1] NCCL INFO Channel 00 : 5[41000] -> 6[81000] via P2P/IPC/read
4: nid001029:130775:130842 [0] NCCL INFO Channel 00 : 3[c1000] -> 4[2000] [receive] via NET/IB/1
3: nid001028:129497:129564 [3] NCCL INFO Channel 00 : 3[c1000] -> 4[2000] [send] via NET/IB/1
4: nid001029:130775:130842 [0] NCCL INFO Channel 00 : 4[2000] -> 5[41000] via P2P/IPC/read
0: nid001028:129494:129556 [0] NCCL INFO Channel 00 : 7[c1000] -> 0[2000] [receive] via NET/IB/1
2: nid001028:129496:129565 [2] NCCL INFO Channel 00 : 2[81000] -> 1[41000] via P2P/IPC/read
3: 
3: nid001028:129497:129564 [3] misc/ibvwrap.cc:268 NCCL WARN Call to ibv_create_cq failed
3: nid001028:129497:129564 [3] NCCL INFO transport/net_ib.cc:338 -> 2
3: nid001028:129497:129564 [3] NCCL INFO transport/net_ib.cc:435 -> 2
3: nid001028:129497:129564 [3] NCCL INFO include/net.h:21 -> 2
3: nid001028:129497:129564 [3] NCCL INFO transport/net.cc:161 -> 2
3: nid001028:129497:129564 [3] NCCL INFO transport.cc:68 -> 2
3: nid001028:129497:129564 [3] NCCL INFO init.cc:766 -> 2
3: nid001028:129497:129564 [3] NCCL INFO init.cc:840 -> 2
3: nid001028:129497:129564 [3] NCCL INFO group.cc:73 -> 2 [Async thread]
0: nid001028:129494:129556 [0] NCCL INFO Channel 00 : 0[2000] -> 1[41000] via P2P/IPC/read
7: nid001029:130778:130844 [3] NCCL INFO Channel 00 : 7[c1000] -> 0[2000] [send] via NET/IB/1
7: 
7: nid001029:130778:130844 [3] misc/ibvwrap.cc:268 NCCL WARN Call to ibv_create_cq failed
7: nid001029:130778:130844 [3] NCCL INFO transport/net_ib.cc:338 -> 2
7: nid001029:130778:130844 [3] NCCL INFO transport/net_ib.cc:435 -> 2
7: nid001029:130778:130844 [3] NCCL INFO include/net.h:21 -> 2
7: nid001029:130778:130844 [3] NCCL INFO transport/net.cc:161 -> 2
7: nid001029:130778:130844 [3] NCCL INFO transport.cc:68 -> 2
7: nid001029:130778:130844 [3] NCCL INFO init.cc:766 -> 2
7: nid001029:130778:130844 [3] NCCL INFO init.cc:840 -> 2
7: nid001029:130778:130844 [3] NCCL INFO group.cc:73 -> 2 [Async thread]
6: nid001029:130777:130847 [2] NCCL INFO Channel 00 : 6[81000] -> 5[41000] via P2P/IPC/read
5: nid001029:130776:130843 [1] NCCL INFO Channel 00 : 5[41000] -> 4[2000] via P2P/IPC/read
3: Traceback (most recent call last):
3:   File "test_ddp.py", line 94, in <module>
3:     main()
3:   File "test_ddp.py", line 70, in main
3:     model = DistributedDataParallel(model, device_ids=device_ids)
3:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 446, in __init__
3:     self._sync_params_and_buffers(authoritative_rank=0)
3:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 457, in _sync_params_and_buffers
3:     self._distributed_broadcast_coalesced(
3:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1155, in _distributed_broadcast_coalesced
3:     dist._broadcast_coalesced(
3: RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1614378083779/work/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled system error, NCCL version 2.7.8
3: ncclSystemError: System call (socket, malloc, munmap, etc) failed.
3: 
1: nid001028:129495:129563 [1] NCCL INFO Channel 00 : 4[2000] -> 1[41000] [receive] via NET/IB/1
1: nid001028:129495:129563 [1] NCCL INFO Channel 00 : 1[41000] -> 0[2000] via P2P/IPC/read
7: Traceback (most recent call last):
7:   File "test_ddp.py", line 94, in <module>
7:     main()
7:   File "test_ddp.py", line 70, in main
7:     model = DistributedDataParallel(model, device_ids=device_ids)
7:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 446, in __init__
7:     self._sync_params_and_buffers(authoritative_rank=0)
7:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 457, in _sync_params_and_buffers
7:     self._distributed_broadcast_coalesced(
7:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1155, in _distributed_broadcast_coalesced
7:     dist._broadcast_coalesced(
7: RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1614378083779/work/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled system error, NCCL version 2.7.8
7: ncclSystemError: System call (socket, malloc, munmap, etc) failed.
7: 
0: 
0: nid001028:129494:129556 [0] include/socket.h:416 NCCL WARN Net : Connection closed by remote peer
0: nid001028:129494:129556 [0] NCCL INFO include/socket.h:438 -> 2
0: nid001028:129494:129556 [0] NCCL INFO include/socket.h:450 -> 2
0: nid001028:129494:129556 [0] NCCL INFO transport/net_ib.cc:479 -> 2
0: nid001028:129494:129556 [0] NCCL INFO include/net.h:22 -> 2
0: nid001028:129494:129556 [0] NCCL INFO transport/net.cc:178 -> 2
0: nid001028:129494:129556 [0] NCCL INFO transport.cc:79 -> 2
0: nid001028:129494:129556 [0] NCCL INFO init.cc:766 -> 2
0: nid001028:129494:129556 [0] NCCL INFO init.cc:840 -> 2
0: nid001028:129494:129556 [0] NCCL INFO group.cc:73 -> 2 [Async thread]
0: Traceback (most recent call last):
0:   File "test_ddp.py", line 94, in <module>
0:     main()
0:   File "test_ddp.py", line 70, in main
0:     model = DistributedDataParallel(model, device_ids=device_ids)
0:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 446, in __init__
0:     self._sync_params_and_buffers(authoritative_rank=0)
0:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 457, in _sync_params_and_buffers
0:     self._distributed_broadcast_coalesced(
0:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1155, in _distributed_broadcast_coalesced
0:     dist._broadcast_coalesced(
0: RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1614378083779/work/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled system error, NCCL version 2.7.8
0: ncclSystemError: System call (socket, malloc, munmap, etc) failed.
0: 
4: 
4: nid001029:130775:130842 [0] include/socket.h:416 NCCL WARN Net : Connection closed by remote peer
4: nid001029:130775:130842 [0] NCCL INFO include/socket.h:438 -> 2
4: nid001029:130775:130842 [0] NCCL INFO include/socket.h:450 -> 2
4: nid001029:130775:130842 [0] NCCL INFO transport/net_ib.cc:479 -> 2
4: nid001029:130775:130842 [0] NCCL INFO include/net.h:22 -> 2
4: nid001029:130775:130842 [0] NCCL INFO transport/net.cc:178 -> 2
4: nid001029:130775:130842 [0] NCCL INFO transport.cc:79 -> 2
4: nid001029:130775:130842 [0] NCCL INFO init.cc:766 -> 2
4: nid001029:130775:130842 [0] NCCL INFO init.cc:840 -> 2
4: nid001029:130775:130842 [0] NCCL INFO group.cc:73 -> 2 [Async thread]
4: Traceback (most recent call last):
4:   File "test_ddp.py", line 94, in <module>
4:     main()
4:   File "test_ddp.py", line 70, in main
4:     model = DistributedDataParallel(model, device_ids=device_ids)
4:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 446, in __init__
4:     self._sync_params_and_buffers(authoritative_rank=0)
4:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 457, in _sync_params_and_buffers
4:     self._distributed_broadcast_coalesced(
4:   File "/global/common/software/nersc/cos1.3/pytorch/1.8.0/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1155, in _distributed_broadcast_coalesced
4:     dist._broadcast_coalesced(
4: RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1614378083779/work/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled system error, NCCL version 2.7.8
4: ncclSystemError: System call (socket, malloc, munmap, etc) failed.
4: 
srun: error: nid001029: task 7: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=8596.0
0: slurmstepd: error: *** STEP 8596.0 ON nid001028 CANCELLED AT 2021-06-01T20:48:18 ***
srun: error: nid001028: task 3: Exited with exit code 1
srun: error: nid001029: task 4: Exited with exit code 1
srun: error: nid001028: task 0: Exited with exit code 1
srun: error: nid001029: task 5: Terminated
srun: error: nid001029: task 6: Terminated
srun: error: nid001028: tasks 1-2: Terminated
srun: Force Terminated StepId=8596.0
